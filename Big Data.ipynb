{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    " - Functions from description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return math.exp(x)/(math.exp(x) + 1) \n",
    "\n",
    "def f(u,v,b):\n",
    "    return -math.log(sigma(u+b)) - math.log(sigma(v+b)) - math.log(sigma(-u/2 - v/2 - b)) + (u**2 + v**2 + b**2)/100\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient of f is a vector (list in python) consisting of three partial derivatives. Each partial derivative is on one of coordinate of the point (u,v,b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partial derivative differentiating on u     \n",
    "def dfu(u,v,b):\n",
    "    return ((u-25)*math.exp(1/2*(u+v+2*b)) + (u+25)*math.exp(1/2*(3*u+v+4*b))+ u * math.exp(u+b) + u - 50) / (50 * (math.exp(u+b)+1) * (math.exp(1/2*(u+v+2*b)) + 1))   \n",
    "\n",
    "#Partial derivative differentiating on v \n",
    "def dfv(u,v,b):\n",
    "    return ((v-25)*math.exp(1/2*(u+v+2*b)) + (v+25)*math.exp(1/2*(3*v+u+4*b))+ v * math.exp(v+b) + v - 50) / (50 * (math.exp(v+b)+1) * (math.exp(1/2*(u+v+2*b)) + 1))\n",
    "\n",
    "#Partial derivative differentiating on b \n",
    "def dfb(u,v,b):\n",
    "    return ((b-50) * math.exp(1/2*(u+v+2*b)) + b * math.exp(u+v+2*b) + b * math.exp(1/2*(3*u+v+4*b)) + b * math.exp(1/2*(u+3*v+4*b)) + (b + 50) * math.exp(3/2*(u+v+2*b)) + (b-50)*math.exp(u+b) + + (b-50)*math.exp(v+b) + b -100) / (50 * (math.exp(u+b) + 1) * (math.exp(v+b) + 1) * (math.exp(1/2*(u+v+2*b)) + 1))\n",
    "\n",
    "def grad_f(u,v,b):\n",
    "    return [dfu(u,v,b), dfv(u,v,b), dfb(u,v,b)]\n",
    "\n",
    "\n",
    "#Alternative approach - testing purposes - More likely to work\n",
    "# - Write only the derivatives of the terms (which is very convenient using sigma) and let the code compute the result of the partial derivative\n",
    "def alt_grad_f(u,v,b):\n",
    "    a_dfu = -sigma(-(u+b)) - 0 - sigma((u+v+2*b)/2)/2 + u/50\n",
    "\n",
    "    a_dfv = -sigma(-(v+b)) - 0 - sigma((u+v+2*b)/2)/2 + v/50\n",
    "\n",
    "    a_dfb = -sigma(-(u+b)) - sigma(-(v+b)) - sigma((u+v+2*b)/2)/2 + b/50\n",
    "\n",
    "    return [a_dfu, a_dfv, a_dfb]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gradient_descent(f, grad_f, eta, u_0, v_0, b_0, max_iter=100):\n",
    "    curX = [u_0, v_0, b_0]  #Current point coordinates\n",
    "    best = 1000000000 #Initial best score\n",
    "    min_t = 1 #Initial best step\n",
    "    t = 0\n",
    "    while t < max_iter: \n",
    "        t = t + 1\n",
    "        step_size = eta(t) #Get step size at each iteration\n",
    "        #print(step_size,t)\n",
    "        #Calculate the new points\n",
    "        gradient_result  = grad_f(curX[0],curX[1],curX[2]) \n",
    "        for i in range (3):\n",
    "            curX[i]= curX[i] - step_size * gradient_result[i]\n",
    "        \n",
    "        #Check whether f(u_t,v_t,b_t) is the smaller than the current smallest result\n",
    "        cur_f = f(curX[0],curX[1],curX[2])\n",
    "        #print(cur_f)\n",
    "        if(cur_f < best): \n",
    "            best = cur_f\n",
    "            min_t = t\n",
    "\n",
    "    curX.append(best)\n",
    "    curX.append(min_t) #return u_100, v_100, b_100, smallest result and its t\n",
    "    return curX \n",
    "\n",
    "#Step-size functions\n",
    "def eta_const(t, c=0.2):\n",
    "    return c\n",
    "\n",
    "def eta_sqrt(t,c=0.5):\n",
    "    return c / math.sqrt(t+1)\n",
    "\n",
    "def eta_multistep(t, milestones, c, eta_init):\n",
    "    i = 0\n",
    "    while i < len(milestones):\n",
    "        if(t < milestones[i]):\n",
    "            return eta_init * (c**i)\n",
    "        i+=1\n",
    "    return eta_init * (c**i)\n",
    "\n",
    "#Initial points\n",
    "u_0 = 4\n",
    "v_0 = 2\n",
    "b_0 = 1\n",
    "\n",
    "#Print result\n",
    "def print_result(result):\n",
    "    print(result)\n",
    "\n",
    "    u_100 = result[0]\n",
    "    v_100 = result[1]\n",
    "    b_100 = result[2]\n",
    "\n",
    "    answer = f(u_100, v_100, b_100)\n",
    "    print(\"f(u_100, v_100, b_100) = \", answer)\n",
    "    print(\"Min val\", result[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.932842186096249, 9.625582043296216, 8.961378701816436, 4.479956715059489, 1]\n",
      "f(u_100, v_100, b_100) =  22.165442593433415\n",
      "Min val 4.479956715059489\n"
     ]
    }
   ],
   "source": [
    "#Part A\n",
    "result = gradient_descent(f, alt_grad_f, lambda t: eta_const(t, c=0.2) ,u_0, v_0, b_0, max_iter=100)\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.406197691237842, 5.772105632130208, 4.941526931915652, 4.632814070457192, 1]\n",
      "f(u_100, v_100, b_100) =  12.656591557514515\n",
      "Min val 4.632814070457192\n"
     ]
    }
   ],
   "source": [
    "#Part B\n",
    "result = gradient_descent(f, alt_grad_f, lambda t: eta_sqrt(t, c=0.5) ,u_0, v_0, b_0, max_iter=100)\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17.194178576256242, 16.476043825123245, 16.108700658312436, 5.29121534994458, 1]\n",
      "f(u_100, v_100, b_100) =  41.20971219838906\n",
      "Min val 5.29121534994458\n"
     ]
    }
   ],
   "source": [
    "#Print C\n",
    "result = gradient_descent(f, alt_grad_f, lambda t: eta_multistep(t, milestones=[30,50,80], c=0.5, eta_init=1.0) ,u_0, v_0, b_0, max_iter=100)\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
