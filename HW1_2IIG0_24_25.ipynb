{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d9e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eadd07",
   "metadata": {},
   "source": [
    "If possible, update your sklearn version to 1.3.2 to reduce variance in the versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7826208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install scikit-learn==1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be50de0",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134bfe32",
   "metadata": {},
   "source": [
    "- Functions from description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return math.exp(x)/(math.exp(x) + 1) \n",
    "\n",
    "def f(u,v,b):\n",
    "    return -math.log(sigma(u+b)) - math.log(sigma(v+b)) - math.log(sigma(-u/2 - v/2 - b)) + (u**2 + v**2 + b**2)/100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc14974",
   "metadata": {},
   "source": [
    "- Gradient of f is a vector (list in python) consisting of three partial derivatives. Each partial derivative is on one of coordinate of the point (u,v,b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e815987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write only the derivatives of the terms (which is very convenient using sigma) and let the code compute the result of the partial derivative\n",
    "def alt_grad_f(u,v,b):\n",
    "    a_dfu = -sigma(-(u+b)) - 0 + sigma((u+v+2*b)/2)/2 + u/50\n",
    "\n",
    "    a_dfv = -sigma(-(v+b)) - 0 + sigma((u+v+2*b)/2)/2 + v/50\n",
    "\n",
    "    a_dfb = -sigma(-(u+b)) - sigma(-(v+b)) + sigma((u+v+2*b)/2) + b/50\n",
    "\n",
    "    return [a_dfu, a_dfv, a_dfb]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cbe290",
   "metadata": {},
   "source": [
    "Gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9278b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, grad_f, eta, u_0, v_0, b_0, max_iter=100):\n",
    "    curX = [u_0, v_0, b_0]  #Current point coordinates\n",
    "    best = 1000000000 #Initial best score\n",
    "    min_t = 1 #Initial best step\n",
    "    t = 0\n",
    "    while t < max_iter: \n",
    "        t = t + 1\n",
    "        step_size = eta(t) #Get step size at each iteration\n",
    "        #print(step_size,t)\n",
    "        #Calculate the new points\n",
    "        gradient_result  = grad_f(curX[0],curX[1],curX[2]) \n",
    "        for i in range (3):\n",
    "            curX[i]= curX[i] - step_size * gradient_result[i]\n",
    "        \n",
    "        #Check whether f(u_t,v_t,b_t) is the smaller than the current smallest result\n",
    "        cur_f = f(curX[0],curX[1],curX[2])\n",
    "        #print(cur_f)\n",
    "        if(cur_f < best): \n",
    "            best = cur_f\n",
    "            min_t = t\n",
    "\n",
    "    curX.append(best)\n",
    "    curX.append(min_t) #return u_100, v_100, b_100, smallest result and its t\n",
    "    return curX \n",
    "\n",
    "#Step-size functions\n",
    "def eta_const(t, c=0.2):\n",
    "    return c\n",
    "\n",
    "def eta_sqrt(t,c=0.5):\n",
    "    return c / math.sqrt(t+1)\n",
    "\n",
    "def eta_multistep(t, milestones, c, eta_init):\n",
    "    i = 0\n",
    "    while i < len(milestones):\n",
    "        if(t < milestones[i]):\n",
    "            return eta_init * (c**i)\n",
    "        i+=1\n",
    "    return eta_init * (c**i)\n",
    "\n",
    "#Initial points\n",
    "u_0 = 4\n",
    "v_0 = 2\n",
    "b_0 = 1\n",
    "\n",
    "#Print result\n",
    "def print_result(result):\n",
    "    #print(result)\n",
    "\n",
    "    u_100 = result[0]\n",
    "    v_100 = result[1]\n",
    "    b_100 = result[2]\n",
    "\n",
    "    answer = f(u_100, v_100, b_100)\n",
    "    print(\"f(u_100, v_100, b_100) = \", round(answer,3))\n",
    "    print(\"Min val\", round(result[3],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e5972",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c969c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part A\n",
    "result = gradient_descent(f, alt_grad_f, lambda t: eta_const(t, c=0.2) ,u_0, v_0, b_0, max_iter=100)\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2623bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part B\n",
    "result = gradient_descent(f, alt_grad_f, lambda t: eta_sqrt(t, c=0.5) ,u_0, v_0, b_0, max_iter=100)\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e094a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print C\n",
    "result = gradient_descent(f, alt_grad_f, lambda t: eta_multistep(t, milestones=[30,50,80], c=0.5, eta_init=1.0) ,u_0, v_0, b_0, max_iter=100)\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb6571",
   "metadata": {},
   "source": [
    "## Coordinate Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19b9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return math.exp(x[0]-x[1]+1) + math.exp(x[1]-x[2]+2) + math.exp(x[2]-x[0]+3)\n",
    "\n",
    "#Partial derivative = 0 for x1\n",
    "def argmin_x1(x):\n",
    "    return  (x[2] + x[1] + 2)/2   #(x3 + x2 + 2) / 2\n",
    "\n",
    "#Partial derivative = 0 for x2\n",
    "def argmin_x2(x):\n",
    "    return (x[2] + x[0] - 1)/2    #(x3 + x1 - 1) / 2\n",
    "\n",
    "#Partial derivative = 0 for x3\n",
    "def argmin_x3(x):\n",
    "    return (x[0] + x[1] - 1)/2    #(x1 + x2 - 1) / 2\n",
    "\n",
    "def argmin_result(x):\n",
    "    return [argmin_x1(x),argmin_x2(x),argmin_x3(x)]\n",
    "\n",
    "x0 = [2,3,4]\n",
    "print(argmin_result(x0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b57e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_descent(f, argmin, x_0, max_iter=100):\n",
    "    x_t = x_0\n",
    "    t = 0\n",
    "    while t < max_iter:\n",
    "        t+=1\n",
    "        for i in range (3):\n",
    "            x_t[i] = argmin[i](x_t)\n",
    "            print(x_t[i],t) #print coordinates and iteration\n",
    "\n",
    "        #Print result of iteration\n",
    "        cur_f = f(x_t)\n",
    "        print(cur_f)\n",
    "    return x_t\n",
    "\n",
    "x_0 = [1,20,5]\n",
    "argmin = [argmin_x1,argmin_x2,argmin_x3] \n",
    "coordinate_descent(f, argmin, x_0, max_iter=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c1c5d2",
   "metadata": {},
   "source": [
    "## Regression - Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df6fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "california = fetch_california_housing()\n",
    "print(california.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9fa714",
   "metadata": {},
   "source": [
    "Creating the data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = california.data\n",
    "y = california.target\n",
    "n,d = D.shape\n",
    "print(n,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff613f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "D_scaled = scaler.fit_transform(D)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02092543",
   "metadata": {},
   "source": [
    "Creating a design matrix with polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b5fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2,include_bias=True)\n",
    "X = poly.fit_transform(D_scaled)\n",
    "poly.get_feature_names_out(california.feature_names)\n",
    "print(\"Original feature shape:\", D.shape)\n",
    "print(\"Design matrix shape with polynomial features (degree 2):\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce394fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in  range(len(poly.get_feature_names_out(california.feature_names))):\n",
    "    if poly.get_feature_names_out(california.feature_names)[i] == 'MedInc':\n",
    "        print('MedInc: ' + str(i))\n",
    "    if poly.get_feature_names_out(california.feature_names)[i] == 'MedInc AveBedrms':\n",
    "        print('MedInc AveBedrms: ' + str(i))\n",
    "    if poly.get_feature_names_out(california.feature_names)[i] == 'HouseAge AveBedrms':\n",
    "        print('HouseAge AveBedrms:' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ae0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "XT_X = X.T @ X\n",
    "beta = (np.linalg.inv(XT_X) @ X.T) @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e8b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta[1], beta[12], beta[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacdc0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.1\n",
    "I = np.eye(X.shape[1])\n",
    "XT_y = X.T @ y\n",
    "\n",
    "ridge_beta = np.linalg.inv(XT_X + lambda_ * n * I) @ XT_y\n",
    "ridge_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce22cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(ridge_beta[1],5), round(ridge_beta[12],5), round(ridge_beta[19],5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a9829",
   "metadata": {},
   "source": [
    "## Bias-var Trade Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac07be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#True regression function\n",
    "# pls appear\n",
    "def f_star(x):\n",
    "    return math.tan(math.pi*x)\n",
    "\n",
    "#Regression models\n",
    "def f_d1(x):\n",
    "    return x+0.2\n",
    "\n",
    "def f_d2(x):\n",
    "    return 3*x+0.3\n",
    "\n",
    "def f_d3(x):\n",
    "    return 5*x+0.1\n",
    "\n",
    "#Calculate expected result from the sample models with x0 = 0 and use it to get the bias^2\n",
    "e_fd = (f_d1(0) + f_d2(0) + f_d3(0)) / 3 #using the hint\n",
    "bias_2 = round((f_star(0) - e_fd) ** 2,4)\n",
    "#print(\"E = \", e_fd)\n",
    "print(\"Answers\")\n",
    "print(\"bias^2 = \", bias_2)\n",
    "\n",
    "#Using the formula from the slides (just the standard way of computing variance)\n",
    "ee_fd = ((e_fd - f_d1(0))**2 + (e_fd - f_d2(0))**2 + (e_fd - f_d3(0)))/3\n",
    "variance = round(ee_fd **2,5)\n",
    "#print(e_fd - f_d3(0))\n",
    "#print(\"EE = \", ee_fd)\n",
    "print(\"variance = \", variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e04e4b",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "From the 20Newsgroups dataset we fetch the documents belonging to three categories, which we use as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6475d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da785a92",
   "metadata": {},
   "source": [
    "For example, the first document in the training data is the following one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93d9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1af6f97",
   "metadata": {},
   "source": [
    "The classes are indicated categorically with indices from zero to two by the target vector. The target names tell us which index belongs to which class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ceeda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.target\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad3956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebde71a",
   "metadata": {},
   "source": [
    "We represent the documents in a bag of word format. That is, we create a data matrix ``D`` such that ``D[j,i]=1`` if the j-th document contains the i-th feature (word), and ``D[j,i]=0`` otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167204f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", min_df=5,token_pattern=\"[^\\W\\d_]+\", binary=True)\n",
    "D = vectorizer.fit_transform(train.data)\n",
    "D_test = vectorizer.transform(test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9fdf2b",
   "metadata": {},
   "source": [
    "We get the allocation of feature indices to words by the following array, containing the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee18ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d57b38",
   "metadata": {},
   "source": [
    "For example, the word `naive` has the index 4044."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ace193",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_id = np.where(vectorizer.get_feature_names_out() == 'naive')[0]\n",
    "naive_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470801fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = len([a for a in y_train if a == 0])/len(y_train)\n",
    "y1 = len([a for a in y_train if a == 1])/len(y_train)\n",
    "y2 = len([a for a in y_train if a == 2])/len(y_train)\n",
    "y3 = len([a for a in y_train if a == 3])/len(y_train)\n",
    "\n",
    "y0, y1, y2, y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb04ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-5  # Smoothing parameter\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "V = len(vocabulary)  # Vocabulary size\n",
    "unique_classes = np.unique(y_train)\n",
    "\n",
    "log_probabilities = {}\n",
    "\n",
    "for k in unique_classes:\n",
    "    # Get indices of documents in class k\n",
    "    class_indices = np.where(y_train == k)[0]\n",
    "    \n",
    "    # Number of documents in class k\n",
    "    N_k = len(class_indices)\n",
    "    \n",
    "    # Number of documents in class k containing the word \"naive\"\n",
    "    N_naive_k = D[class_indices, naive_id].sum()\n",
    "    \n",
    "    # Compute smoothed probability\n",
    "    prob = (N_naive_k + alpha) / (N_k + alpha * V)\n",
    "    log_probabilities[k] = np.log(prob)\n",
    "\n",
    "# Print log-probabilities\n",
    "for k, log_prob in log_probabilities.items():\n",
    "    print(f\"log p(x_naive = 1 | y = {k}) = {log_prob:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the following values are known or calculated: 5C\n",
    "# 1. Priors (p(y=k))\n",
    "class_priors = [0.25, 0.25, 0.25, 0.25]  # Replace with actual priors from 5a\n",
    "\n",
    "# 2. Likelihoods (p(x_word=1 | y=k)) from 5b\n",
    "likelihoods = {\n",
    "    'autos': [0.001, 0.0001, 0.0001, 0.0001],\n",
    "    'motorcycles': [0.0001, 0.002, 0.0001, 0.0001],\n",
    "    'baseball': [0.0001, 0.0001, 0.003, 0.0001],\n",
    "    'hockey': [0.0001, 0.0001, 0.0001, 0.004],\n",
    "}\n",
    "\n",
    "# Function to compute posterior probabilities using Bayes' theorem\n",
    "def compute_posterior(word, class_priors, likelihoods):\n",
    "    # Extract likelihoods for the given word\n",
    "    word_likelihoods = likelihoods[word]\n",
    "    \n",
    "    # Compute the denominator p(x_word=1)\n",
    "    p_x_word = sum(word_likelihoods[k] * class_priors[k] for k in range(len(class_priors)))\n",
    "    \n",
    "    # Compute posterior probabilities for each class\n",
    "    posteriors = [(word_likelihoods[k] * class_priors[k]) / p_x_word for k in range(len(class_priors))]\n",
    "    return posteriors\n",
    "\n",
    "# Example: Compute for each word\n",
    "words = ['autos', 'motorcycles', 'baseball', 'hockey']\n",
    "for word in words:\n",
    "    posteriors = compute_posterior(word, class_priors, likelihoods)\n",
    "    for k, posterior in enumerate(posteriors):\n",
    "        print(f\"p(y = {k} | x_{word} = 1) = {posterior:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daf68a3",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6962d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "D, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aca608",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a56f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_imp(y):\n",
    "    temp = 0\n",
    "    for i in range(3):\n",
    "        temp += (len([a for a in y if a == i])/len(y))**2\n",
    "    g_impurity = 1 - temp\n",
    "    return g_impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f615356",
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_imp(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0264a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "D[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f001a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sep_id = []\n",
    "big_sep_id = []\n",
    "for i in range(150):\n",
    "    if(D[i][0] <= 5.84):\n",
    "        small_sep_id.append(i)\n",
    "    else:\n",
    "        big_sep_id.append(i)\n",
    "#print(small_sep_id)\n",
    "#print(big_sep_id)\n",
    "y_small = [y[j] for j in small_sep_id]\n",
    "y_big = [y[j] for j in big_sep_id]\n",
    "#print(gini_imp(y_small))\n",
    "#print(gini_imp(y_big))\n",
    "cost = (len(small_sep_id)/150 * gini_imp(y_small) + len(big_sep_id)/150 * gini_imp(y_big)) - gini_imp(y)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e24eba",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7946aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Label %i\" % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ceaddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the images\n",
    "n = len(digits.images)\n",
    "D = digits.images.reshape((n, -1))\n",
    "y = digits.target\n",
    "\n",
    "# Split data into 70% train and 30% test subsets\n",
    "D_train, D_test, y_train, y_test = train_test_split(\n",
    "    D, y, test_size=0.3, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='rbf', gamma=0.0008, C=0.9)\n",
    "model = svc.fit(D_train, y_train)\n",
    "print(svc.score(D_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2177cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many supporting vectors classes 0 and 1 have in total\n",
    "support_vectors = svc.n_support_\n",
    "all_support_vectors_0_1 = support_vectors[0] + support_vectors[1]\n",
    "all_support_vectors_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cfeea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute how many supporting vectors are distinguished between classes 0 and 1\n",
    "all_coeficients_0_1 = model.dual_coef_[0, :all_support_vectors_0_1]\n",
    "distinguished_coeficients_0_1 = all_coeficients_0_1[np.where(all_coeficients_0_1 != 0)]\n",
    "distinguished_coeficients_0_1.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c715b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract support vectors and dual coefficients from training\n",
    "support_vectors = svc.support_vectors_\n",
    "dual_coef = svc.dual_coef_\n",
    "\n",
    "# Separate the support vectors for class 0 and class 1\n",
    "# Dual coefficients have two rows for binary classification\n",
    "class_0_indices = (dual_coef[0] > 0).nonzero()[0]\n",
    "class_1_indices = (dual_coef[0] < 0).nonzero()[0]\n",
    "\n",
    "# Get the top 4 most influential support vectors for each class\n",
    "top_4_class_0 = support_vectors[class_0_indices[:4]]\n",
    "top_4_class_1 = support_vectors[class_1_indices[:4]]    \n",
    "\n",
    "\n",
    "# Plot the support vectors\n",
    "fig, axes = plt.subplots(2, 4, figsize=(10, 5))\n",
    "for i, sv in enumerate(top_4_class_0):\n",
    "    axes[0, i].imshow(sv.reshape(8, 8), cmap='gray')\n",
    "    axes[0, i].set_title(f\"Class 0, SV {i+1}\")\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "for i, sv in enumerate(top_4_class_1):\n",
    "    axes[1, i].imshow(sv.reshape(8, 8), cmap='gray')\n",
    "    axes[1, i].set_title(f\"Class 1, SV {i+1}\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Top 4 Support Vectors for Classes 0 and 1\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db4b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SVC with RBF kernel\n",
    "svc = SVC()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'kernel':['rbf'],\n",
    "    'gamma': [0.0001, 0.0006, 0.001, 0.006],\n",
    "    'C': [0.6, 0.8, 1, 2, 3, 4, 6]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "\n",
    "# Train the model on the whole dataset D\n",
    "grid_search.fit(D, y)\n",
    "\n",
    "# Get the best parameters and the best accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
